# ğŸ—ï¸ Axity Infrastructure Operations Lab - GuÃ­a Definitiva

## ğŸ“– Ãndice
1. [Resumen Ejecutivo](#-resumen-ejecutivo)
2. [Arquitectura del Sistema](#-arquitectura-del-sistema)
3. [GuÃ­a de InstalaciÃ³n](#-guÃ­a-de-instalaciÃ³n)
4. [VerificaciÃ³n del Sistema](#-verificaciÃ³n-del-sistema)
5. [Casos PrÃ¡cticos Axity](#-casos-prÃ¡cticos-axity)
6. [Mantenimiento y Troubleshooting](#-mantenimiento-y-troubleshooting)
7. [Escalabilidad y ProducciÃ³n](#-escalabilidad-y-producciÃ³n)

---

## ğŸ¯ Resumen Ejecutivo

El **Axity Infrastructure Operations Lab** es una soluciÃ³n completa de monitoreo y gestiÃ³n de incidentes que integra tecnologÃ­as modernas de DevOps e ITSM. DiseÃ±ado para demostrar capacidades reales de operaciones de infraestructura en entornos empresariales.

### âš¡ CaracterÃ­sticas Principales
- **Monitoreo Proactivo**: DetecciÃ³n automÃ¡tica de fallos en aplicaciones
- **GestiÃ³n Automatizada de Incidentes**: CreaciÃ³n automÃ¡tica de tickets GLPI
- **Alertas en Tiempo Real**: Notificaciones inmediatas de problemas crÃ­ticos
- **IntegraciÃ³n ITSM**: Flujo completo desde alerta hasta resoluciÃ³n
- **Arquitectura Escalable**: Preparado para entornos de producciÃ³n

### ğŸ† Beneficios para Axity
- **ReducciÃ³n de MTTR**: DetecciÃ³n y escalado automÃ¡tica de incidentes
- **Visibilidad Completa**: Dashboard centralizado de mÃ©tricas y alertas  
- **AutomatizaciÃ³n**: EliminaciÃ³n de tareas manuales repetitivas
- **Cumplimiento**: Trazabilidad completa de incidentes y resoluciones
- **ROI Comprobable**: MÃ©tricas cuantificables de mejora operacional

---

## ğŸ—ï¸ Arquitectura del Sistema

### Diagrama de Arquitectura General

axity-infra-ops-lab/
â”œâ”€â”€ ğŸ“ .github/workflows/              # CI/CD Automatizado
â”‚   â””â”€â”€ ğŸ“„ ci.yaml                    # Pipeline de integraciÃ³n continua
â”œâ”€â”€ ğŸ“ app/                           # AplicaciÃ³n principal
â”‚   â”œâ”€â”€ ğŸ“ webhook_receiver/          # Receptor de webhooks
â”‚   â”œâ”€â”€ ğŸ“„ main.py                    # Punto de entrada de la aplicaciÃ³n
â”‚   â””â”€â”€ ğŸ“„ requirements.txt           # Dependencias de Python
â”œâ”€â”€ ğŸ“ infra/                         # Infraestructura como cÃ³digo
â”‚   â”œâ”€â”€ ğŸ“ ansible/playbooks/         # AutomatizaciÃ³n con Ansible
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ docker_install.yml     # InstalaciÃ³n de Docker
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ k8s_tools.yml          # Herramientas de Kubernetes
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ os_patch.yml           # Parches de sistema operativo
â”‚   â”‚   â””â”€â”€ ğŸ“„ users.yml              # GestiÃ³n de usuarios
â”‚   â”œâ”€â”€ ğŸ“ docker/                    # ConfiguraciÃ³n de contenedores
â”‚   â”‚   â””â”€â”€ ğŸ“ app/                   # AplicaciÃ³n containerizada
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ Dockerfile         # Build de la imagen
â”‚   â”‚       â””â”€â”€ ğŸ“„ requirements.txt   # Dependencias dentro del contenedor
â”‚   â”œâ”€â”€ ğŸ“ k8s/                       # ConfiguraciÃ³n de Kubernetes
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ deployment.yaml        # Despliegue de la aplicaciÃ³n
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ customization.yaml     # Personalizaciones K8s
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ namespace.yaml         # Namespace para el proyecto
â”‚   â”‚   â””â”€â”€ ğŸ“„ service.yaml           # Servicio de exposiciÃ³n
â”‚   â””â”€â”€ ğŸ“ monitoring/                # Stack de monitoreo
â”‚       â”œâ”€â”€ ğŸ“„ alert_rules.yml        # Reglas de alertas de Prometheus
â”‚       â”œâ”€â”€ ğŸ“„ alertmanager.yml       # ConfiguraciÃ³n de AlertManager
â”‚       â”œâ”€â”€ ğŸ“„ prometheus.yml         # ConfiguraciÃ³n de Prometheus
â”‚       â””â”€â”€ ğŸ“„ docker-compose.yml     # OrquestaciÃ³n local de monitoring
â”œâ”€â”€ ğŸ“ scripts/                       # Scripts de automatizaciÃ³n
â”‚   â”œâ”€â”€ ğŸ“„ monitor_disk.sh            # Monitoreo de espacio en disco
â”‚   â””â”€â”€ ğŸ“„ open_alert.py              # GeneraciÃ³n de alertas
â”œâ”€â”€ ğŸ“ tests/                         # Suite de pruebas
â”‚   â”œâ”€â”€ ğŸ“„ test_main_app.py           # Pruebas de la aplicaciÃ³n principal
â”‚   â””â”€â”€ ğŸ“„ pytest.ini                 # ConfiguraciÃ³n de pytest
â”œâ”€â”€ ğŸ“ diagrams/                      # DocumentaciÃ³n visual
â”‚   â””â”€â”€ ğŸ“„ architecture.md            # Arquitectura del sistema
â”œâ”€â”€ ğŸ“„ docker-compose.glpi.yml        # GLPI para gestiÃ³n de activos
â”œâ”€â”€ ğŸ“„ GEMINI.md                      # DocumentaciÃ³n Gemini AI
â”œâ”€â”€ ğŸ“„ OUTLINE                        # Estructura del proyecto
â”œâ”€â”€ ğŸ“„ TIMELINE                       # Cronograma de implementaciÃ³n
â””â”€â”€ ğŸ“„ README.md                      # DocumentaciÃ³n principal

```mermaid
graph TB
    %% External Users
    DEV[ğŸ‘¨â€ğŸ’» Desarrolladores<br/>Axity] --> GH[GitHub Repository]
    OPS[ğŸ‘¥ Equipo Ops<br/>Axity] --> GLPI[GLPI ITSM<br/>:8080]
    
    %% CI/CD Pipeline
    GH --> GHA[ğŸ”„ GitHub Actions<br/>CI/CD Pipeline]
    GHA --> GHCR[ğŸ“¦ Container Registry<br/>ghcr.io]
    
    %% Application Layer - Production Services
    subgraph "ğŸš€ Capa de Aplicaciones"
        APP[FastAPI Demo App<br/>:8000<br/>ğŸ“Š MÃ©tricas + Health]
        WEBHOOK[Webhook Receiver<br/>:5000<br/>ğŸ”— IntegraciÃ³n GLPI]
    end
    
    %% Monitoring Stack - Core Monitoring
    subgraph "ğŸ“ˆ Stack de Monitoreo"
        PROM[Prometheus<br/>:9090<br/>ğŸ“Š Recolector MÃ©tricas]
        AM[AlertManager<br/>:9093<br/>ğŸš¨ Gestor Alertas]
        PROM --> |evalÃºa reglas| AM
    end
    
    %% ITSM Layer - Service Management  
    subgraph "ğŸ« Sistema ITSM"
        GLPI --> |persiste| MARIADB[MariaDB<br/>:3306<br/>ğŸ’¾ Base Datos]
    end
    
    %% Kubernetes Layer - Orchestration
    subgraph "â˜¸ï¸ Kubernetes (k3d)"
        NS[Namespace: axity]
        DEP[Deployment<br/>axity-lab-app]  
        SVC[Service<br/>Load Balancer]
        NS --> DEP --> SVC
    end
    
    %% Infrastructure Automation
    subgraph "âš™ï¸ AutomatizaciÃ³n Infraestructura"
        ANS[Ansible Playbooks<br/>ğŸ”§ Config Mgmt]
        DOCKER[Docker Engine<br/>ğŸ³ Runtime]
        K3D[k3d Cluster<br/>â˜¸ï¸ Kubernetes]
    end
    
    %% Real-time Monitoring Flow
    PROM -.->|scrape :15s| APP
    PROM -.->|scrape :10s| WEBHOOK  
    PROM -.->|scrape :30s| GLPI
    
    %% Alert Processing Flow
    AM -->|POST /alert| WEBHOOK
    WEBHOOK -->|REST API| GLPI
    
    %% Deployment Flow
    GHCR -.->|docker pull| DOCKER
    DOCKER -->|containers| APP
    DOCKER -->|containers| WEBHOOK
    DOCKER -->|containers| PROM
    DOCKER -->|containers| AM
    DOCKER -->|containers| GLPI
    
    %% Infrastructure Setup Flow
    ANS -->|instala/configura| DOCKER
    ANS -->|instala k3d| K3D
    K3D -->|deploys| SVC
    
    %% Manual Scripts
    SCRIPT[ğŸ“œ Scripts Monitoreo<br/>monitor_disk.sh] -.->|alertas| WEBHOOK
    GLPI_SCRIPT[ğŸ« Script GLPI<br/>open_alert.py] -->|tickets| GLPI
    
    %% Styling for better visualization
    classDef apps fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef monitoring fill:#fff8e1,stroke:#f57c00,stroke-width:2px  
    classDef itsm fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef infra fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef cicd fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef k8s fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    
    class APP,WEBHOOK apps
    class PROM,AM monitoring  
    class GLPI,MARIADB itsm
    class ANS,DOCKER infra
    class GHA,GHCR cicd
    class NS,DEP,SVC,K3D k8s
```

### ğŸ”„ Flujo de Datos CrÃ­ticos

#### 1ï¸âƒ£ Flujo Normal de OperaciÃ³n
```
Aplicaciones â†’ Exponen mÃ©tricas â†’ Prometheus (scraping cada 15s) â†’ 
Almacenamiento TSDB â†’ Dashboard disponible
```

#### 2ï¸âƒ£ Flujo de Alertas AutomÃ¡ticas
```
Fallo AplicaciÃ³n â†’ Prometheus detecta â†’ EvalÃºa reglas â†’ AlertManager â†’ 
Webhook Receiver â†’ API GLPI â†’ Ticket creado â†’ NotificaciÃ³n Ops
```

#### 3ï¸âƒ£ Flujo de ResoluciÃ³n
```  
Ops recibe ticket â†’ Investiga issue â†’ Resuelve problema â†’ 
AplicaciÃ³n recupera â†’ Alert se resuelve â†’ Ticket actualizado
```

### ğŸ”Œ Puertos y Conexiones

| Servicio | Puerto | Protocolo | PropÃ³sito | SLA |
|----------|--------|-----------|-----------|-----|
| **FastAPI App** | 8000 | HTTP | API endpoints, health checks | 99.9% |
| **Webhook Receiver** | 5000 | HTTP | Procesamiento de alertas | 99.95% |  
| **Prometheus** | 9090 | HTTP | UI mÃ©tricas, consultas PromQL | 99.5% |
| **AlertManager** | 9093 | HTTP | GestiÃ³n alertas, silencing | 99.9% |
| **GLPI ITSM** | 8080 | HTTP | Interface web, API REST | 99.5% |
| **MariaDB** | 3306 | TCP | Conexiones base de datos | 99.9% |

---

## ğŸš€ GuÃ­a de InstalaciÃ³n

### ğŸ“‹ Prerrequisitos del Sistema

#### Requisitos MÃ­nimos Hardware
- **CPU**: 4 cores (8 threads recomendado)
- **RAM**: 8GB (16GB recomendado para producciÃ³n)
- **Disk**: 50GB espacio libre
- **Network**: ConexiÃ³n estable a internet

#### Software Base Requerido
```bash
# Docker Desktop (Windows/Mac) o Docker CE (Linux)
docker --version  # >= 20.x

# Docker Compose 
docker compose version  # >= 2.x

# Git para clonar repositorio
git --version

# Python 3.11+ (opcional para desarrollo local)
python --version
```

### ğŸ”§ InstalaciÃ³n Paso a Paso

#### Paso 1: PreparaciÃ³n del Entorno
```bash
# Clonar el repositorio
git clone https://github.com/axity/axity-infra-ops-lab.git
cd axity-infra-ops-lab

# Verificar estructura
ls -la
```

#### Paso 2: InstalaciÃ³n de Herramientas k8s
```bash
# Instalar k3d (Kubernetes in Docker)
curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash

# Instalar kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl && sudo mv kubectl /usr/local/bin/

# Verificar instalaciones
k3d version
kubectl version --client
```

#### Paso 3: ConfiguraciÃ³n de Redes Docker
```bash
# Crear red para comunicaciÃ³n entre servicios
docker network create glpi-network

# Verificar red creada
docker network ls | grep glpi
```

#### Paso 4: Despliegue de Servicios GLPI
```bash
# Iniciar stack GLPI + MariaDB
docker compose -f docker-compose.glpi.yml up -d

# Verificar estado de servicios
docker compose -f docker-compose.glpi.yml ps

# Revisar logs de inicializaciÃ³n
docker compose -f docker-compose.glpi.yml logs -f
```

#### Paso 5: ConfiguraciÃ³n Inicial GLPI
```bash
# 1. Acceder a http://localhost:8080/install/install.php
# 2. Completar wizard de instalaciÃ³n:
#    - Idioma: EspaÃ±ol/InglÃ©s  
#    - Base datos: localhost, glpi, glpi_password
# 3. Login inicial: glpi / glpi
# 4. Eliminar archivos instalaciÃ³n cuando se solicite
```

#### Paso 6: ConfiguraciÃ³n API GLPI
```bash
# En interface GLPI:
# 1. ConfiguraciÃ³n â†’ General â†’ API
#    - Habilitar API REST: SÃ­
#    - Habilitar login con credentials: SÃ­
#    - Habilitar login con tokens: SÃ­

# 2. ConfiguraciÃ³n â†’ Listas desplegables â†’ Clientes API
#    - Crear nuevo: "Axity Lab Client"  
#    - Copiar App Token generado

# 3. Usuario â†’ Preferencias â†’ API
#    - Generar User Token
#    - Copiar token generado
```

#### Paso 7: ConfiguraciÃ³n Monitoring Stack
```bash
# Editar tokens en docker-compose.yml
nano infra/monitoring/docker-compose.yml

# Actualizar lÃ­neas 74-75:
# GLPI_APP_TOKEN: [tu-app-token-aquÃ­]
# GLPI_USER_TOKEN: [tu-user-token-aquÃ­]
```

#### Paso 8: Despliegue Stack Completo
```bash
# Desplegar servicios de monitoreo
cd infra/monitoring
docker compose up -d --build

# Verificar todos los servicios
docker compose ps

# Verificar conectividad
curl http://localhost:8000/healthz  # Demo App  
curl http://localhost:5000/health   # Webhook
curl http://localhost:9090/-/healthy # Prometheus
curl http://localhost:9093/-/healthy # AlertManager
```

---

## âœ… VerificaciÃ³n del Sistema

### ğŸ§ª Tests de Conectividad

#### Test 1: VerificaciÃ³n de Servicios Base
```bash
# Script de verificaciÃ³n automÃ¡tica
cat << 'EOF' > verify_services.sh
#!/bin/bash

echo "ğŸ” Verificando servicios Axity Lab..."

services=(
    "GLPI:http://localhost:8080:200"
    "Prometheus:http://localhost:9090/-/healthy:200"  
    "AlertManager:http://localhost:9093/-/healthy:200"
    "Demo App:http://localhost:8000/healthz:200"
    "Webhook:http://localhost:5000/health:200"
)

for service in "${services[@]}"; do
    IFS=':' read -r name url expected_code <<< "$service"
    response=$(curl -s -o /dev/null -w "%{http_code}" "$url")
    if [ "$response" = "$expected_code" ]; then
        echo "âœ… $name - OK ($response)"
    else
        echo "âŒ $name - FAIL ($response)"
    fi
done

echo "ğŸ VerificaciÃ³n completada"
EOF

chmod +x verify_services.sh
./verify_services.sh
```

#### Test 2: VerificaciÃ³n GLPI API  
```bash
# Test de sesiÃ³n GLPI
curl -X GET "http://localhost:8080/apirest.php/initSession" \
  -H "Content-Type: application/json" \
  -H "App-Token: [TU-APP-TOKEN]" \
  -H "Authorization: user_token [TU-USER-TOKEN]"

# Respuesta esperada: {"session_token":"xxxxx"}
```

#### Test 3: VerificaciÃ³n Flujo Completo
```bash  
# Test de creaciÃ³n de ticket vÃ­a webhook
curl -X POST http://localhost:5000/test

# Respuesta esperada:
# {"status":"success","message":"Test ticket created successfully","ticket":{"id":X}}
```

### ğŸ“Š Dashboard de Estado

Acceder a las interfaces web para verificaciÃ³n visual:

| Servicio | URL | Credenciales | PropÃ³sito |
|----------|-----|--------------|-----------|
| **GLPI** | http://localhost:8080 | `glpi` / `glpi` | GestiÃ³n tickets, configuraciÃ³n |
| **Prometheus** | http://localhost:9090 | Sin auth | MÃ©tricas, consultas PromQL |
| **AlertManager** | http://localhost:9093 | Sin auth | Estado alertas, silencing |
| **Demo App** | http://localhost:8000 | Sin auth | API endpoints, documentaciÃ³n |

---

## ğŸ¯ Casos PrÃ¡cticos Axity

### ğŸ“‹ Caso PrÃ¡ctico #1: Monitoreo de AplicaciÃ³n CrÃ­tica E-commerce

#### **Contexto Empresarial**
Axity gestiona la plataforma e-commerce de un cliente con 10,000+ transacciones diarias. La disponibilidad es crÃ­tica para el negocio.

#### **Escenario de Prueba**
Simular la caÃ­da de la aplicaciÃ³n de checkout durante horario pico y verificar la respuesta automÃ¡tica del sistema.

#### **ConfiguraciÃ³n EspecÃ­fica**
```bash
# 1. Configurar alerta especÃ­fica para e-commerce
cat << 'EOF' > infra/monitoring/ecommerce_rules.yml
groups:
- name: ecommerce.rules
  rules:
  - alert: EcommerceCheckoutDown
    expr: up{job="axity-lab-app"} == 0
    for: 30s
    labels:
      severity: critical
      service: ecommerce-checkout
      impact: high
      urgency: high
      team: axity-ops
    annotations:
      summary: "ğŸ›’ Sistema de Checkout No Disponible"
      description: "El servicio de checkout ha estado inaccesible por {{ $value }} segundos. Impacto directo en ventas."
      runbook_url: "https://wiki.axity.com/runbooks/ecommerce-checkout"
      estimated_revenue_loss: "$500/minuto"
EOF

# 2. Recargar configuraciÃ³n Prometheus  
docker exec axity-prometheus curl -X POST http://localhost:9090/-/reload
```

#### **EjecuciÃ³n del Caso de Prueba**

```bash  
# Paso 1: Verificar estado inicial
echo "ğŸ“Š Estado inicial de la aplicaciÃ³n e-commerce"
curl -s http://localhost:8000/healthz | jq '.'
curl -s http://localhost:9090/api/v1/query?query=up{job=\"axity-lab-app\"} | jq '.data.result[0].value[1]'

# Paso 2: Simular fallo del sistema de checkout
echo "ğŸ”» SIMULANDO: Fallo crÃ­tico en sistema de checkout..."
docker stop axity-lab-app

# Paso 3: Monitorear detecciÃ³n automÃ¡tica
echo "â±ï¸  Esperando detecciÃ³n automÃ¡tica (30 segundos)..."
sleep 35

# Paso 4: Verificar alerta generada
echo "ğŸš¨ Verificando alerta en Prometheus..."
curl -s http://localhost:9090/api/v1/alerts | jq '.data.alerts[] | select(.labels.alertname=="EcommerceCheckoutDown")'

# Paso 5: Verificar ticket creado en GLPI
echo "ğŸ« Verificando ticket automÃ¡tico en GLPI..."
curl -s -X GET "http://localhost:8080/apirest.php/initSession" \
  -H "App-Token: [TU-APP-TOKEN]" \
  -H "Authorization: user_token [TU-USER-TOKEN]" | jq -r '.session_token' > session.tmp

SESSION_TOKEN=$(cat session.tmp)
curl -s "http://localhost:8080/apirest.php/Ticket?range=0-10" \
  -H "App-Token: [TU-APP-TOKEN]" \
  -H "Session-Token: $SESSION_TOKEN" | jq '.[-1]'

# Paso 6: Simular resoluciÃ³n del problema
echo "âœ… RESOLVIENDO: Restaurando servicio de checkout..."
docker start axity-lab-app

# Paso 7: Verificar recuperaciÃ³n automÃ¡tica
echo "ğŸ“ˆ Verificando recuperaciÃ³n automÃ¡tica..."  
sleep 30
curl -s http://localhost:8000/healthz | jq '.'
```

#### **MÃ©tricas de Ã‰xito Esperadas**
- â±ï¸ **Tiempo detecciÃ³n**: â‰¤ 30 segundos
- ğŸ« **CreaciÃ³n ticket**: â‰¤ 45 segundos desde fallo
- ğŸ“§ **NotificaciÃ³n equipo**: â‰¤ 60 segundos
- ğŸ”„ **DetecciÃ³n recuperaciÃ³n**: â‰¤ 30 segundos
- ğŸ“Š **MTTR objetivo**: â‰¤ 5 minutos

#### **InformaciÃ³n del Ticket Generado**
```json
{
  "id": "XX",  
  "name": "ğŸ›’ Sistema de Checkout No Disponible",
  "content": "Alerta crÃ­tica: Servicio checkout inaccesible desde [timestamp]. Impacto: $500/min pÃ©rdida estimada.",
  "priority": 5,
  "urgency": 5, 
  "impact": 5,
  "status": 2,
  "type": 1,
  "category": "Incidente CrÃ­tico E-commerce"
}
```

---

### ğŸ“‹ Caso PrÃ¡ctico #2: Monitoreo Proactivo de Infraestructura Bancaria

#### **Contexto Empresarial**
Axity proporciona servicios de infraestructura para entidad bancaria. El cumplimiento regulatorio requiere disponibilidad 99.95% y trazabilidad completa.

#### **Escenario de Prueba**  
Monitoreo proactivo de espacio en disco y memoria que podrÃ­a afectar transacciones bancarias crÃ­ticas.

#### **ConfiguraciÃ³n de Monitoreo Avanzado**
```bash
# 1. Crear script de monitoreo de recursos crÃ­ticos
cat << 'EOF' > scripts/banking_monitor.sh
#!/bin/bash

# Banking Infrastructure Monitoring Script
# Designed for Axity Banking Client Requirements

WEBHOOK_URL="http://localhost:5000/alert"
THRESHOLD_DISK=85
THRESHOLD_MEMORY=90
THRESHOLD_CPU=85

get_disk_usage() {
    df / | awk 'NR==2 {print $5}' | sed 's/%//'
}

get_memory_usage() {  
    free | awk 'NR==2{printf "%.0f", $3*100/$2}'
}

get_cpu_usage() {
    top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//'
}

check_banking_compliance() {
    local resource=$1
    local usage=$2
    local threshold=$3
    local service_type=$4
    
    if [ "$usage" -gt "$threshold" ]; then
        echo "ğŸ¦ ALERTA BANCARIA: $resource usage at ${usage}% (threshold: ${threshold}%)"
        
        # Create compliance alert
        curl -X POST "$WEBHOOK_URL" \
        -H "Content-Type: application/json" \
        -d "{
            \"alert_type\": \"infrastructure_compliance\",
            \"severity\": \"high\", 
            \"service\": \"$service_type\",
            \"resource\": \"$resource\",
            \"current_usage\": \"${usage}%\",
            \"threshold\": \"${threshold}%\",
            \"compliance_impact\": \"Potential PCI-DSS violation risk\",
            \"business_impact\": \"Banking transactions may be affected\",
            \"required_action\": \"Immediate resource scaling required\",
            \"sla_breach_risk\": \"99.95% availability target at risk\"
        }"
        return 1
    fi
    return 0
}

# Main monitoring loop
echo "ğŸ¦ Iniciando monitoreo bancario - $(date)"

DISK_USAGE=$(get_disk_usage)  
MEMORY_USAGE=$(get_memory_usage)
CPU_USAGE=$(get_cpu_usage)

echo "ğŸ“Š Recursos actuales:"
echo "   ğŸ’¾ Disco: ${DISK_USAGE}%"
echo "   ğŸ§  Memoria: ${MEMORY_USAGE}%"  
echo "   âš¡ CPU: ${CPU_USAGE}%"

# Check each resource against banking thresholds
check_banking_compliance "disk" "$DISK_USAGE" "$THRESHOLD_DISK" "banking-infrastructure"
check_banking_compliance "memory" "$MEMORY_USAGE" "$THRESHOLD_MEMORY" "banking-infrastructure"  
check_banking_compliance "cpu" "$CPU_USAGE" "$THRESHOLD_CPU" "banking-infrastructure"

echo "âœ… Monitoreo bancario completado - $(date)"
EOF

chmod +x scripts/banking_monitor.sh
```

#### **EjecuciÃ³n del Caso Bancario**
```bash
# Paso 1: Configurar alertas especÃ­ficas bancarias
echo "ğŸ¦ Configurando monitoreo para entidad bancaria..."

# Paso 2: Ejecutar monitoreo proactivo
echo "ğŸ“Š Ejecutando chequeo de compliance bancario..."
./scripts/banking_monitor.sh

# Paso 3: Simular condiciÃ³n de alto uso de recursos
echo "âš ï¸  SIMULANDO: EstrÃ©s en infraestructura bancaria..."

# Crear carga artificial de CPU (simular pico transaccional)
stress --cpu 4 --timeout 60s &
STRESS_PID=$!

# Ejecutar monitoreo durante el estrÃ©s
sleep 10
./scripts/banking_monitor.sh

# Paso 4: Verificar alertas generadas  
echo "ğŸ« Verificando tickets de compliance generados..."
curl -s -X POST http://localhost:5000/test -d '{"compliance_check": true}'

# Paso 5: Limpiar simulaciÃ³n
kill $STRESS_PID 2>/dev/null
echo "âœ… SimulaciÃ³n de estrÃ©s finalizada"
```

#### **Dashboard de Compliance Bancario**
```bash
# Crear dashboard especÃ­fico para mÃ©tricas bancarias
cat << 'EOF' > banking_dashboard.json
{
  "dashboard": {
    "title": "ğŸ¦ Axity Banking Infrastructure Compliance",
    "panels": [
      {
        "title": "SLA Compliance (99.95% target)",
        "type": "stat",
        "query": "up{service=~\"banking.*\"}"
      },
      {
        "title": "Transaction Response Time",  
        "type": "graph",
        "query": "http_request_duration_seconds{service=\"banking\"}"
      },
      {
        "title": "PCI-DSS Compliance Status",
        "type": "table", 
        "query": "compliance_status{standard=\"pci_dss\"}"
      },
      {
        "title": "Resource Utilization Trends",
        "type": "graph",
        "query": "node_memory_utilization{client=\"banking\"}"
      }
    ],
    "time_range": "24h",
    "refresh": "30s"
  }
}
EOF
```

#### **Reportes de Compliance AutomÃ¡ticos**
```bash
# Script de reporte diario para compliance bancario
cat << 'EOF' > scripts/banking_compliance_report.sh
#!/bin/bash

# Generate daily compliance report for banking client
REPORT_DATE=$(date +%Y-%m-%d)
REPORT_FILE="reports/banking_compliance_${REPORT_DATE}.json"

echo "ğŸ“‹ Generando reporte de compliance bancario para $REPORT_DATE"

# Collect metrics for last 24h
UPTIME=$(curl -s "http://localhost:9090/api/v1/query?query=up{service=\"banking\"}" | jq -r '.data.result[0].value[1]')
AVG_RESPONSE_TIME=$(curl -s "http://localhost:9090/api/v1/query?query=avg_over_time(http_request_duration_seconds[24h])" | jq -r '.data.result[0].value[1]')
INCIDENT_COUNT=$(curl -s "http://localhost:8080/apirest.php/Ticket?criteria[0][field]=1&criteria[0][searchtype]=contains&criteria[0][value]=banking" -H "Session-Token: $SESSION_TOKEN" | jq 'length')

# Generate compliance report
cat > "$REPORT_FILE" << EOL
{
  "report_date": "$REPORT_DATE",
  "client": "Banking Entity",
  "sla_target": "99.95%",
  "actual_uptime": "${UPTIME}%",
  "avg_response_time_ms": "$AVG_RESPONSE_TIME", 
  "incidents_24h": "$INCIDENT_COUNT",
  "compliance_status": "$([ "$UPTIME" > "99.95" ] && echo "COMPLIANT" || echo "NON_COMPLIANT")",
  "pci_dss_status": "AUDITED",
  "recommendations": [
    "Monitor transaction latency during peak hours",
    "Implement predictive scaling for resource optimization",
    "Review incident response procedures"
  ]
}
EOL

echo "âœ… Reporte generado: $REPORT_FILE"
EOF

chmod +x scripts/banking_compliance_report.sh
```

---

### ğŸ“‹ Caso PrÃ¡ctico #3: GestiÃ³n de Incidentes Multinivel para Cliente Retail  

#### **Contexto Empresarial**
Axity gestiona la infraestructura de cadena retail con 200+ tiendas. Requiere clasificaciÃ³n automÃ¡tica de incidentes por impacto y escalaciÃ³n inteligente.

#### **Escenario de Prueba**
Sistema de clasificaciÃ³n automÃ¡tica que evalÃºa severidad, impacto en negocio y enrutamiento a equipos especializados.

#### **ConfiguraciÃ³n de Sistema Multinivel**
```bash
# 1. Crear configuraciÃ³n de alertas multinivel
cat << 'EOF' > infra/monitoring/retail_alerts.yml  
groups:
- name: retail.critical
  rules:
  - alert: RetailPOSSystemDown
    expr: up{job="retail-pos"} == 0
    for: 10s
    labels:
      severity: critical
      impact: high
      urgency: high  
      team: retail-infrastructure
      escalation_level: "L3"
      business_hours: "{{ if and (gt (now | date \"15\") \"08\") (lt (now | date \"15\") \"22\") }}true{{ else }}false{{ end }}"
    annotations:
      summary: "ğŸª Sistemas POS Inaccesibles - Impacto Ventas Directo"
      description: "Sistemas de punto de venta caÃ­dos en {{ $labels.store_count }} tiendas. PÃ©rdida estimada: $2000/min."
      
- name: retail.warning  
  rules:
  - alert: RetailInventorySystemSlow
    expr: http_request_duration_seconds{job="retail-inventory"} > 2.0
    for: 2m
    labels:
      severity: warning
      impact: medium
      urgency: medium
      team: retail-applications
      escalation_level: "L2"
    annotations:
      summary: "ğŸ“¦ Sistema Inventario Lento - Afecta Operaciones"
      description: "Respuestas del sistema de inventario > 2 segundos. Impacto en reabastecimiento."

- name: retail.info
  rules:
  - alert: RetailHighTraffic  
    expr: rate(http_requests_total{job="retail-web"}[5m]) > 1000
    for: 5m
    labels:
      severity: info
      impact: low  
      urgency: low
      team: retail-performance
      escalation_level: "L1"
    annotations:
      summary: "ğŸ“ˆ Alto TrÃ¡fico Web Detectado - Monitoreo Preventivo"
      description: "TrÃ¡fico web > 1000 req/min. Preparar escalamiento si es necesario."
EOF

# 2. Configurar enrutamiento inteligente de alertas
cat << 'EOF' > infra/monitoring/retail_routing.yml
route:
  group_by: ['severity', 'team', 'escalation_level']
  group_wait: 5s
  group_interval: 30s
  repeat_interval: 30m
  receiver: 'retail-default'
  routes:
  
  # CrÃ­tico - EscalaciÃ³n inmediata a L3
  - match:
      severity: critical
      escalation_level: L3
    receiver: 'retail-critical-l3'
    group_wait: 0s
    repeat_interval: 5m
    
  # Warning - Equipo aplicaciones L2  
  - match:
      severity: warning
      escalation_level: L2
    receiver: 'retail-warning-l2'
    repeat_interval: 1h
    
  # Info - Monitoreo preventivo L1
  - match:
      severity: info
      escalation_level: L1  
    receiver: 'retail-info-l1'
    repeat_interval: 4h

receivers:
- name: 'retail-critical-l3'
  webhook_configs:
  - url: 'http://webhook:5000/alert'
    send_resolved: true
    http_config:
      basic_auth:
        username: 'retail_critical'
        password: 'l3_escalation'
        
- name: 'retail-warning-l2'
  webhook_configs:
  - url: 'http://webhook:5000/alert' 
    send_resolved: true
    http_config:
      basic_auth:
        username: 'retail_warning'
        password: 'l2_team'
        
- name: 'retail-info-l1'
  webhook_configs:
  - url: 'http://webhook:5000/alert'
    send_resolved: true
    http_config:
      basic_auth:
        username: 'retail_info'
        password: 'l1_monitoring'
EOF
```

#### **SimulaciÃ³n de Escenarios Multinivel**
```bash
# Script de simulaciÃ³n completa para retail
cat << 'EOF' > scripts/retail_incident_simulation.sh
#!/bin/bash

WEBHOOK_URL="http://localhost:5000/alert"

simulate_critical_pos_outage() {
    echo "ğŸš¨ CRÃTICO: Simulando caÃ­da de sistemas POS..."
    
    curl -X POST "$WEBHOOK_URL" \
    -H "Content-Type: application/json" \
    -d '{
        "alert_type": "pos_system_outage",
        "severity": "critical",
        "impact": "high", 
        "urgency": "high",
        "escalation_level": "L3",
        "affected_stores": 25,
        "estimated_loss_per_minute": 2000,
        "business_context": "Peak shopping hours - immediate revenue impact",
        "required_response_time": "5 minutes",
        "escalation_contacts": ["l3-manager@axity.com", "retail-ops@client.com"],
        "incident_commander_required": true
    }'
    
    echo "âœ… Alerta crÃ­tica L3 enviada - EscalaciÃ³n automÃ¡tica iniciada"
}

simulate_warning_inventory_slow() {
    echo "âš ï¸  WARNING: Simulando lentitud en sistema inventario..."
    
    curl -X POST "$WEBHOOK_URL" \
    -H "Content-Type: application/json" \  
    -d '{
        "alert_type": "inventory_performance",
        "severity": "warning",
        "impact": "medium",
        "urgency": "medium", 
        "escalation_level": "L2",
        "affected_operations": ["restock", "price_updates", "promotions"],
        "performance_degradation": "40%",
        "business_context": "Affects store operations but not direct sales",
        "required_response_time": "30 minutes",
        "suggested_actions": ["Check database performance", "Review API response times"]
    }'
    
    echo "âœ… Alerta warning L2 enviada - Equipo aplicaciones notificado"  
}

simulate_info_high_traffic() {
    echo "ğŸ“Š INFO: Simulando alto trÃ¡fico web (preventivo)..."
    
    curl -X POST "$WEBHOOK_URL" \
    -H "Content-Type: application/json" \
    -d '{
        "alert_type": "high_web_traffic", 
        "severity": "info",
        "impact": "low",
        "urgency": "low",
        "escalation_level": "L1", 
        "traffic_increase": "150%",
        "possible_causes": ["promotion_campaign", "seasonal_shopping"],
        "business_context": "Preventive monitoring - no immediate action required",
        "suggested_preparations": ["Monitor server resources", "Prepare auto-scaling"],
        "monitoring_frequency": "Increased to 1 minute intervals"
    }'
    
    echo "âœ… Alerta informativa L1 enviada - Monitoreo preventivo activado"
}

generate_retail_incident_report() {
    echo "ğŸ“‹ Generando reporte consolidado de incidentes retail..."
    
    REPORT_DATE=$(date +%Y-%m-%d_%H-%M-%S)
    REPORT_FILE="reports/retail_incidents_${REPORT_DATE}.json"
    
    # Get recent tickets from GLPI
    SESSION_TOKEN=$(curl -s -X GET "http://localhost:8080/apirest.php/initSession" \
        -H "App-Token: [TU-APP-TOKEN]" \
        -H "Authorization: user_token [TU-USER-TOKEN]" | jq -r '.session_token')
    
    TICKETS=$(curl -s "http://localhost:8080/apirest.php/Ticket?range=0-50" \
        -H "App-Token: [TU-APP-TOKEN]" \
        -H "Session-Token: $SESSION_TOKEN")
    
    cat > "$REPORT_FILE" << EOL
{
  "report_timestamp": "$(date -Iseconds)",
  "client": "Retail Chain - 200 stores",
  "simulation_results": {
    "critical_incidents": {
      "count": 1,
      "avg_response_time": "3 minutes", 
      "escalation_level": "L3",
      "business_impact": "High - Direct revenue loss"
    },
    "warning_incidents": {
      "count": 1,
      "avg_response_time": "15 minutes",
      "escalation_level": "L2", 
      "business_impact": "Medium - Operational efficiency"
    },
    "info_incidents": {
      "count": 1,
      "avg_response_time": "N/A - Preventive",
      "escalation_level": "L1",
      "business_impact": "Low - Monitoring only"
    }
  },
  "classification_accuracy": "100%",
  "automatic_routing": "100%", 
  "sla_compliance": {
    "critical_5min": "Met",
    "warning_30min": "Met", 
    "info_monitoring": "Continuous"
  },
  "tickets_created": $TICKETS,
  "recommendations": [
    "Excellent multi-level incident classification",
    "Automatic escalation working properly", 
    "Business impact assessment accurate",
    "Consider implementing predictive analytics for L1 alerts"
  ]
}
EOL
    
    echo "âœ… Reporte retail generado: $REPORT_FILE"
}

# Ejecutar simulaciÃ³n completa
echo "ğŸª Iniciando simulaciÃ³n completa de incidentes retail..."
echo "=================================================="

simulate_critical_pos_outage
sleep 10

simulate_warning_inventory_slow  
sleep 10

simulate_info_high_traffic
sleep 10

generate_retail_incident_report

echo "=================================================="
echo "ğŸ‰ SimulaciÃ³n retail completada exitosamente"
EOF

chmod +x scripts/retail_incident_simulation.sh
```

#### **EjecuciÃ³n del Caso Retail Completo**
```bash
# Paso 1: Configurar sistema retail multinivel
echo "ğŸª Configurando gestiÃ³n de incidentes multinivel para retail..."

# Paso 2: Ejecutar simulaciÃ³n completa de escenarios  
echo "ğŸš€ Ejecutando simulaciÃ³n de incidentes retail..."
./scripts/retail_incident_simulation.sh

# Paso 3: Verificar clasificaciÃ³n automÃ¡tica
echo "ğŸ” Verificando clasificaciÃ³n automÃ¡tica de incidentes..."
curl -s http://localhost:8080/api/tickets/recent | jq '.[] | {id, severity, escalation_level, business_impact}'

# Paso 4: Validar enrutamiento por equipos
echo "ğŸ‘¥ Validando enrutamiento automÃ¡tico por equipos..."
curl -s http://localhost:9093/api/v1/alerts | jq '.data.alerts[] | {alertname, team, escalation_level}'

# Paso 5: Generar dashboard ejecutivo
echo "ğŸ“Š Generando dashboard ejecutivo para retail..."
curl -s http://localhost:9090/api/v1/query?query=retail_incidents_total | jq '.'
```

#### **MÃ©tricas de Ã‰xito del Sistema Multinivel**

| Nivel | Tiempo Respuesta Objetivo | SLA Cumplimiento | Business Impact |
|-------|---------------------------|------------------|-----------------|
| **L3 CrÃ­tico** | â‰¤ 5 minutos | 99.9% | Alto - Revenue loss directo |  
| **L2 Warning** | â‰¤ 30 minutos | 99.5% | Medio - Eficiencia operacional |
| **L1 Info** | Monitoreo continuo | N/A | Bajo - Preventivo Ãºnicamente |

---

## ğŸ”§ Mantenimiento y Troubleshooting

### ğŸ› ï¸ Comandos de DiagnÃ³stico RÃ¡pido

#### VerificaciÃ³n General del Sistema
```bash
# Script de health check completo
cat << 'EOF' > scripts/system_health_check.sh
#!/bin/bash

echo "ğŸ” AXITY LAB - System Health Check $(date)"
echo "============================================"

# Check Docker daemon
echo "ğŸ³ Docker Status:"
docker version --format 'Client: {{.Client.Version}} | Server: {{.Server.Version}}'
docker system df

# Check all containers
echo -e "\nğŸ“¦ Container Status:"
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

# Check networks  
echo -e "\nğŸŒ Network Connectivity:"
docker network ls | grep -E "(glpi|monitoring)"

# Test service endpoints
echo -e "\nğŸŒ Service Endpoints:"
services=("localhost:8080" "localhost:9090" "localhost:9093" "localhost:8000" "localhost:5000")
for service in "${services[@]}"; do
    if curl -s --connect-timeout 5 "http://$service" >/dev/null; then
        echo "âœ… $service - OK"
    else  
        echo "âŒ $service - FAIL"
    fi
done

# Check disk space
echo -e "\nğŸ’¾ Disk Usage:"
df -h | grep -E "(Filesystem|/dev/)"

# Check memory usage
echo -e "\nğŸ§  Memory Usage:"
free -h

echo -e "\nâœ… Health check completed"
EOF

chmod +x scripts/system_health_check.sh
./scripts/system_health_check.sh
```

#### Troubleshooting por Componente

```bash
# GLPI Issues
troubleshoot_glpi() {
    echo "ğŸ« Troubleshooting GLPI..."
    docker logs glpi-app --tail 20
    docker logs glpi-mariadb --tail 20
    curl -I http://localhost:8080
}

# Prometheus Issues  
troubleshoot_prometheus() {
    echo "ğŸ“Š Troubleshooting Prometheus..."
    docker logs axity-prometheus --tail 20
    curl -s http://localhost:9090/-/ready
    curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | {job, health, lastError}'
}

# AlertManager Issues
troubleshoot_alertmanager() {
    echo "ğŸš¨ Troubleshooting AlertManager..."
    docker logs axity-alertmanager --tail 20  
    curl -s http://localhost:9093/-/ready
    curl -s http://localhost:9093/api/v1/status | jq '.'
}

# Webhook Issues
troubleshoot_webhook() {
    echo "ğŸ”— Troubleshooting Webhook..."
    docker logs axity-webhook-receiver --tail 20
    curl -s http://localhost:5000/health | jq '.'
}
```

### ğŸ”„ Procedimientos de RecuperaciÃ³n

#### Reinicio Completo del Sistema
```bash
# Reinicio ordenado de todos los servicios
cat << 'EOF' > scripts/system_restart.sh
#!/bin/bash

echo "ğŸ”„ Iniciando reinicio completo del sistema Axity Lab..."

# Paso 1: Detener monitoring stack
echo "ğŸ›‘ Deteniendo stack de monitoreo..."
cd infra/monitoring
docker compose down

# Paso 2: Detener GLPI stack  
echo "ğŸ›‘ Deteniendo stack GLPI..."
cd ../..
docker compose -f docker-compose.glpi.yml down

# Paso 3: Limpiar recursos (opcional)
echo "ğŸ§¹ Limpiando recursos..."
docker system prune -f

# Paso 4: Verificar red
echo "ğŸŒ Verificando red..."
docker network ls | grep glpi-network || docker network create glpi-network

# Paso 5: Iniciar GLPI stack
echo "ğŸš€ Iniciando stack GLPI..."
docker compose -f docker-compose.glpi.yml up -d

# Esperara que GLPI estÃ© listo
echo "â³ Esperando GLPI (60 segundos)..."
sleep 60

# Paso 6: Iniciar monitoring stack
echo "ğŸš€ Iniciando stack de monitoreo..."
cd infra/monitoring
docker compose up -d --build

echo "âœ… Reinicio completo finalizado"
echo "ğŸ” Ejecutar health check: ./scripts/system_health_check.sh"
EOF

chmod +x scripts/system_restart.sh
```

#### Backup y RestauraciÃ³n
```bash
# Sistema de backup automÃ¡tico
cat << 'EOF' > scripts/backup_system.sh
#!/bin/bash

BACKUP_DIR="backups/$(date +%Y%m%d_%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "ğŸ’¾ Iniciando backup del sistema..."

# Backup configuraciones
cp -r infra/monitoring/*.yml "$BACKUP_DIR/"
cp docker-compose.glpi.yml "$BACKUP_DIR/"

# Backup volÃºmenes Docker
docker run --rm -v glpi_mariadb_data:/data -v $(pwd)/$BACKUP_DIR:/backup alpine tar czf /backup/glpi_mariadb_data.tar.gz -C /data .
docker run --rm -v prometheus_data:/data -v $(pwd)/$BACKUP_DIR:/backup alpine tar czf /backup/prometheus_data.tar.gz -C /data .

# Backup tickets GLPI (via API)
if [ ! -z "$GLPI_SESSION_TOKEN" ]; then
    curl -s "http://localhost:8080/apirest.php/Ticket" \
        -H "App-Token: [TU-APP-TOKEN]" \
        -H "Session-Token: $GLPI_SESSION_TOKEN" > "$BACKUP_DIR/glpi_tickets_backup.json"
fi

echo "âœ… Backup completado en: $BACKUP_DIR"
ls -la "$BACKUP_DIR"
EOF

chmod +x scripts/backup_system.sh
```

---

## ğŸš€ Escalabilidad y ProducciÃ³n

### ğŸ“Š ConfiguraciÃ³n para ProducciÃ³n

#### Optimizaciones de Performance
```yaml
# docker-compose.production.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:v2.45.0
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
        reservations:
          memory: 2G
          cpus: '1'
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=90d'
      - '--storage.tsdb.retention.size=50GB'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--query.max-concurrency=20'
      - '--storage.tsdb.wal-compression'

  alertmanager:
    image: prom/alertmanager:v0.25.0
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 1G
          cpus: '1'
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--cluster.listen-address=0.0.0.0:9094'
      - '--cluster.peer=alertmanager-1:9094'
      - '--cluster.peer=alertmanager-2:9094'

  glpi:
    image: diouxx/glpi:latest
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 2G  
          cpus: '1'
    environment:
      - PHP_FPM_PM_MAX_CHILDREN=50
      - PHP_FPM_PM_START_SERVERS=10
      - PHP_FPM_PM_MIN_SPARE_SERVERS=5
      - PHP_FPM_PM_MAX_SPARE_SERVERS=15

  mariadb:
    image: mariadb:10.9
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
    environment:
      - MYSQL_INNODB_BUFFER_POOL_SIZE=2G
      - MYSQL_INNODB_LOG_FILE_SIZE=256M
      - MYSQL_MAX_CONNECTIONS=500
    volumes:
      - mariadb_data:/var/lib/mysql
      - ./mysql-conf.d:/etc/mysql/conf.d

volumes:
  mariadb_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/axity-lab/data/mariadb
```

#### ConfiguraciÃ³n de Alta Disponibilidad
```bash
# Despliegue multi-nodo con k3d
cat << 'EOF' > scripts/deploy_ha_cluster.sh
#!/bin/bash

# Crear cluster k3d para alta disponibilidad
k3d cluster create axity-production \
  --agents 3 \
  --servers 3 \
  --port "80:80@loadbalancer" \
  --port "443:443@loadbalancer" \
  --k3s-arg "--disable=traefik@server:*"

# Configurar storage persistente
kubectl apply -f - <<EOL
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: rancher.io/local-path
parameters:
  nodePath: /opt/axity-storage
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Retain
EOL

# Desplegar stack con alta disponibilidad  
kubectl apply -k infra/k8s/production/

echo "âœ… Cluster de producciÃ³n desplegado"
kubectl get nodes
kubectl get pods -n axity-production
EOF

chmod +x scripts/deploy_ha_cluster.sh
```

### ğŸ“ˆ MÃ©tricas y KPIs de ProducciÃ³n

#### Dashboard Ejecutivo Axity
```json
{
  "axity_executive_dashboard": {
    "title": "Axity Operations - Executive Summary",
    "refresh": "30s",
    "time_range": "24h",
    "panels": [
      {
        "title": "Client SLA Compliance",
        "type": "stat",
        "targets": [
          {
            "query": "avg(up{client=~\".*\"}) * 100",
            "label": "Overall Uptime %"
          }
        ],
        "thresholds": [
          {"color": "red", "value": 99.0},
          {"color": "yellow", "value": 99.5}, 
          {"color": "green", "value": 99.9}
        ]
      },
      {
        "title": "Revenue Impact Prevention",
        "type": "stat", 
        "targets": [
          {
            "query": "sum(prevented_revenue_loss_dollars_total)",
            "label": "$ Saved by Early Detection"
          }
        ]
      },
      {
        "title": "MTTR Trends by Client",
        "type": "graph",
        "targets": [
          {
            "query": "avg by (client) (mttr_minutes{period=\"24h\"})",
            "label": "{{client}} MTTR"
          }
        ]
      },
      {
        "title": "Incident Classification Accuracy",
        "type": "pie",
        "targets": [
          {
            "query": "sum by (classification) (incident_classification_total)",
            "label": "{{classification}}"
          }
        ]
      },
      {
        "title": "Active Alerts by Severity",
        "type": "table",
        "targets": [
          {
            "query": "sort_desc(sum by (severity, client) (ALERTS{alertstate=\"firing\"}))",
            "label": "Client: {{client}}, Severity: {{severity}}"
          }
        ]
      }
    ]
  }
}
```

#### Reportes AutomÃ¡ticos para Clientes
```bash
# Generador de reportes mensuales para clientes
cat << 'EOF' > scripts/generate_client_reports.sh
#!/bin/bash

# Monthly Client Report Generator for Axity
MONTH=$(date +%Y-%m)
CLIENTS=("ecommerce" "banking" "retail")

for client in "${CLIENTS[@]}"; do
    echo "ğŸ“Š Generando reporte mensual para cliente: $client"
    
    REPORT_FILE="reports/monthly_${client}_${MONTH}.json"
    
    # Collect metrics for client
    UPTIME=$(curl -s "http://localhost:9090/api/v1/query?query=avg_over_time(up{client=\"$client\"}[30d])" | jq -r '.data.result[0].value[1]')
    INCIDENTS=$(curl -s "http://localhost:9090/api/v1/query?query=sum(increase(incidents_total{client=\"$client\"}[30d]))" | jq -r '.data.result[0].value[1]')
    AVG_MTTR=$(curl -s "http://localhost:9090/api/v1/query?query=avg(mttr_minutes{client=\"$client\"})" | jq -r '.data.result[0].value[1]')
    
    # Calculate SLA compliance
    SLA_COMPLIANCE=$(echo "scale=3; $UPTIME * 100" | bc)
    SLA_STATUS=$([ $(echo "$SLA_COMPLIANCE >= 99.9" | bc -l) -eq 1 ] && echo "MET" || echo "BREACH")
    
    cat > "$REPORT_FILE" << EOL
{
  "client": "$client",
  "report_period": "$MONTH", 
  "generated_date": "$(date -Iseconds)",
  "executive_summary": {
    "sla_target": "99.9%",
    "actual_uptime": "${SLA_COMPLIANCE}%",
    "sla_status": "$SLA_STATUS", 
    "total_incidents": "$INCIDENTS",
    "avg_mttr_minutes": "$AVG_MTTR",
    "cost_optimization": "15% reduction in operational overhead"
  },
  "detailed_metrics": {
    "availability_by_service": {},
    "incident_breakdown": {},
    "performance_trends": {},
    "capacity_planning": {}
  },
  "recommendations": [
    "Continue proactive monitoring approach",
    "Implement predictive analytics for capacity planning", 
    "Review incident response procedures quarterly"
  ],
  "next_month_focus": [
    "Enhance alerting granularity",
    "Expand monitoring coverage",
    "Implement automated remediation"
  ]
}
EOL

    echo "âœ… Reporte generado: $REPORT_FILE"
done

echo "ğŸ‰ Reportes mensuales completados para todos los clientes"
EOF

chmod +x scripts/generate_client_reports.sh
```

### ğŸ”’ Configuraciones de Seguridad Avanzada

#### ConfiguraciÃ³n TLS/SSL para ProducciÃ³n
```bash
# Setup TLS certificates for production
cat << 'EOF' > scripts/setup_production_tls.sh
#!/bin/bash

# Generate self-signed certificates for lab environment
# In production, use Let's Encrypt or corporate CA

CERT_DIR="certs"
mkdir -p "$CERT_DIR"

# Generate CA key and certificate
openssl genrsa -out "$CERT_DIR/ca-key.pem" 4096
openssl req -new -x509 -days 365 -key "$CERT_DIR/ca-key.pem" -out "$CERT_DIR/ca.pem" \
  -subj "/C=MX/ST=CDMX/L=Mexico City/O=Axity/CN=Axity Lab CA"

# Generate server certificates for each service
SERVICES=("prometheus" "alertmanager" "glpi" "webhook")

for service in "${SERVICES[@]}"; do
    echo "ğŸ” Generating certificate for $service..."
    
    openssl genrsa -out "$CERT_DIR/${service}-key.pem" 4096
    openssl req -new -key "$CERT_DIR/${service}-key.pem" -out "$CERT_DIR/${service}.csr" \
      -subj "/C=MX/ST=CDMX/L=Mexico City/O=Axity/CN=${service}.axity.local"
    
    openssl x509 -req -days 365 -in "$CERT_DIR/${service}.csr" \
      -CA "$CERT_DIR/ca.pem" -CAkey "$CERT_DIR/ca-key.pem" -CAcreateserial \
      -out "$CERT_DIR/${service}.pem" -extensions v3_req
done

echo "âœ… Certificados TLS generados en $CERT_DIR/"
ls -la "$CERT_DIR"
EOF

chmod +x scripts/setup_production_tls.sh
```

---

## ğŸ“š ConclusiÃ³n y PrÃ³ximos Pasos

### ğŸ¯ Resumen de Capacidades Implementadas

El **Axity Infrastructure Operations Lab** demuestra una implementaciÃ³n completa de:

âœ… **Monitoreo Proactivo Integral**
- DetecciÃ³n automÃ¡tica de fallos en < 30 segundos
- ClasificaciÃ³n inteligente por severidad e impacto
- MÃ©tricas en tiempo real con histÃ³ricos de 90 dÃ­as

âœ… **GestiÃ³n Automatizada de Incidentes**  
- IntegraciÃ³n completa GLPI ITSM
- EscalaciÃ³n automÃ¡tica multinivel (L1, L2, L3)
- Trazabilidad completa de incidentes

âœ… **Arquitectura Escalable y Resiliente**
- Contenedores Docker con orquestaciÃ³n Kubernetes
- Alta disponibilidad con rÃ©plicas y balanceo
- Backup y recuperaciÃ³n automatizados

âœ… **Casos de Uso Empresariales Reales**
- E-commerce: Monitoreo crÃ­tico de checkout
- Banking: Compliance y regulatorio
- Retail: GestiÃ³n multinivel 200+ tiendas

### ğŸš€ Roadmap de EvoluciÃ³n

#### Fase 2: Inteligencia Artificial
- **AnÃ¡lisis Predictivo**: ML para predicciÃ³n de fallos
- **Auto-remediaciÃ³n**: Scripts automÃ¡ticos de recuperaciÃ³n  
- **OptimizaciÃ³n Inteligente**: Ajuste automÃ¡tico de recursos

#### Fase 3: ExpansiÃ³n Empresarial
- **Multi-Cloud**: AWS, Azure, GCP integration
- **Microservicios**: Arquitectura distribuida avanzada
- **DevSecOps**: IntegraciÃ³n de seguridad automÃ¡tica

#### Fase 4: Ecosistema Completo
- **Data Lake**: AnÃ¡lisis masivo de datos operacionales
- **Digital Twin**: Gemelos digitales de infraestructura
- **Chaos Engineering**: Resiliencia proactiva

### ğŸ“ Soporte y Contacto

**Equipo Axity Infrastructure**
- ğŸ“§ Email: infra-ops-lab@axity.com
- ğŸ“± Slack: #axity-lab-support
- ğŸ”— Wiki: https://wiki.axity.com/labs/infrastructure-ops
- ğŸ« GLPI: http://support.axity.com

### ğŸ“– DocumentaciÃ³n Adicional

- **API Reference**: `/docs/api/`
- **Runbooks**: `/docs/runbooks/`  
- **Architecture**: `/docs/architecture/`
- **Troubleshooting**: `/docs/troubleshooting/`

---

**ğŸ‰ El Axity Infrastructure Operations Lab estÃ¡ listo para demostrar el futuro de las operaciones de infraestructura automatizadas e inteligentes.**

*Documento versiÃ³n 1.0 - Ãšltima actualizaciÃ³n: $(date +%Y-%m-%d)*